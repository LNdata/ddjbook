:chapnum: 04
:figure-number: 00

== Obtener datos ==

image::figs/incoming/04-00-cover.png[float="none",role="informal"]

++++
<?dbfo-need height="1in"?>
++++


Así que está listo para comenzar con su primer proyecto de periodismo de
datos. ¿Y ahora qué? Primero necesita algunos datos. Esta sección
analiza de dónde puede obtenerlos. Aquí aprenderemos cómo encontrar
datos en la red, cómo pedirlos usando las leyes de acceso a la
información, cómo usar el "screen scraping" (peinado de pantalla) para
recoger datos de fuentes no estructuradas, y cómo usar la “colaboración
del público” (crowdsourcing) para obtener sus propios conjuntos de datos
de sus lectores. Finalmente analizamos lo que dicen las leyes respecto
de la re-edición de conjuntos de datos, y cómo usar herramientas legales
simples para permitir a otros reutilizar sus datos.


=== Una guía para trabajos de campo de 5 minutos

¿Busca datos sobre un tópico o cuestión particular? ¿No está seguro de
qué es lo que hay o dónde encontrarlo? ¿No sabe por dónde empezar? En
esta sección analizamos cómo comenzar la búsqueda de fuentes de datos
públicos en la red.

==== Ajustar la búsqueda

Aunque pueden no ser siempre fáciles de encontrar, muchas bases de datos
en la red están indexadas por motores de búsqueda, fuera ello o no la
intención del editor. Unos cuantos consejos:

   * Cuando busque datos asegúrese de incluir tanto términos de búsqueda
relacionados con el contenido de los datos que trata de encontrar, como
algo de información sobre el formato o la fuente en la que prevé
encontrarlos. Google y otros motores de búsqueda le permiten buscar por
tipo de archivo. Por ejemplo, puede buscar solo planillas de cálculo
(agregando a su búsqueda “filetype:XLS filetype:CSV”), datos geográficos
(“filetype:shp”), o extractos de bases de datos (“filetype:MDB,
filetype:SQL, filetype:DB). Si así lo desea incluso puede buscar PDF
(“filetype:pdf”).
   * También puede buscar con una parte de una URL. Hacer una búsqueda en
Google de “inurl:donwloads filetype:xls” o “inurl:descargas
filetype:xls” significa buscar todos los archivos Excel que tienen
“downloads” o “descargas” en su dirección de la red (si encuentra una
sola descarga, a menudo vale la pena simplemente verificar qué otros
resultados existen para la misma carpeta en el servidor de la red).
También puede limitar su búsqueda solo a aquellos resultados dentro de
un solo nombre de dominio, buscando “site:agency.gov”, por ejemplo.
   * Otro truco popular es no buscar determinado contenido directamente,
sino lugares donde puede haber datos disponibles en gran cantidad. Por
ejemplo “sitio:ente.gov Directory Listing” puede darle algunos listados
generados por el servidor de la red con fácil acceso a archivos en
bruto, mientas que “sitio:ente.gov Database Download” buscará listados
creados intencionalmente.

++++
<?dbfo-need height="2in"?>
++++

.Ir Directo a la fuente
****
El primer truco que uso para obtener datos que están en manos de un ente
público es tratar de ir directo a quien tiene los datos, no la persona
de relaciones públicas, ni a través de un pedido de acceso a la
información (PAI). Podría por supuesto hacer un PAI o un pedido de
registros públicos, pero eso hace que los engranajes comiencen a girar
con lentitud. Es probable que reciba la respuesta de que los datos no
están en el formato que solicité o (tal como ha sucedido en algunos
casos) que el ente oficial usa un software propio y no puede extraer los
datos en el formato que requerí. Pero si empiezo por llegar a la persona
que maneja los datos para esa organización, puedo hacer preguntas
respecto de qué datos tienen sobre el tema y cómo los guardan. Puedo
conocer el formato, hablar en el lenguaje de los datos y descubrir lo
que necesito saber para pedir los datos y tener éxito. ¿Las barreras que
se enfrentan en este caso? A menudo es difícil llegar a estas personas.
El encargado/a de Información Pública (EIP) va a querer que trate
directamente con él/ella. En esos casos he descubierto que lo mejor es
tratar de organizar una llamada colectiva o, aún mejor, una reunión en
persona con el/la EIP, el gurú de datos, y yo. Y lo puedo organizar de
un modo que les resulte difícil decir que no. “No quiero darles
trabajo”, digo. “No quiero crear una carga innecesaria ni hacer un
pedido demasiado amplio, de modo que una reunión me ayudará a entender
exactamente lo que tienen y cómo pedir exactamente lo que necesito”.

Si este método no funciona, la alternativa es hacer un pedido de conocer
cómo está organizado su archivo y su diccionario de datos. Entonces pido
los datos efectivamente. A veces pregunto también cómo guardan y qué
sistema usan. De ese modo puedo investigar de qué modo exportar los
datos antes de escribir mi pedido.

Por último, mi mejor historia de éxito es de cuando estaba trabajando en
un pequeño diario en Montana. Necesitaba algunos datos de países, me
dijeron que no podían exportarse de la computadora central. Investigué
un poco y ofrecí ir a ayudarlos. Trabajé con la persona de datos,
hicimos un pequeño guión y copiamos los datos a un disquete (esto fue
hace mucho tiempo). Tenía mis datos y el condado ahora estaba en
condiciones de proveer los datos a cualquiera que los pidiera. No
querían que eso sucediera, pero a veces ellos también necesitaban
extraer datos y no entendían su sistema por completo, de modo que nos
ayudamos entre todos.

&mdash; _Cheryl Philips, The Seattle Times_
****

==== Explore sitios y servicios de datos

En los últimos años han aparecido una cantidad de portales y centros de
datos dedicados y otros sitios de datos en la red. Son buenos lugares
para llegar a conocer los tipos de datos que hay. Para empezar podría
ver:

[[FIG042]]
.datacatalogs.org (Open Knowledge Foundation)
image::figs/incoming/04-01.png[float="0"]

Portales oficiales de datos::
  La disposición del gobierno a entregar ciertos conjuntos de datos varía
de país en país. Un número creciente de países está lanzando portales de
datos (inspirados por el data.gov de EE.UU. y el data.gov.uk del R.U.)
para promover la reutilización civil y comercial de información oficial.
Se puede encontrar un índice global actualizado de tales sitios en
http://datacatalogs.org/[datacatalogs.org/]. Otro sitio práctico
es el http://www.guardian.co.uk/world-government-data[Guardian World Government Data],
un meta-motor de búsquedas que incluye muchos catálogos de datos
gubernamentales internacionales.


http://thedatahub.org/[El Data Hub]::

  Un recurso comunitario manejado por la Open Knowledge Foundation que
facilita buscar, compartir y re-utilizar fuentes de datos abiertamente
disponibles, especialmente de maneras automatizadas.

https://scraperwiki.com/[Scraperwiki]::

Una herramienta online para hacer que el proceso de extraer “datos
útiles sea más fácil de modo que puedan ser utilizados en otras
aplicaciones o que periodistas e investigadores puedan _scrapear_ en
ellos”. La mayoría de los "scrapers" y sus bases de datos son públicos y
pueden ser reutilizados.

Portales de datos del http://data.worldbank.org/[Banco Mundial]  y las
http://data.un.org/[Naciones Unidas]::

Estos servicios ofrecen indicadores de alto nivel para todos los países
y en muchos casos cubren muchos años.

http://buzzdata.com/[Buzzdata], http://www.infochimps.com/[Infochimps]
y http://datamarket.com/[DataMarket]::

Sitios nuevos que apuntan a crear comunidades dedicadas a compartir
datos y su reventa.

http://datacouch.com/[DataCouch]::

Un lugar donde subir, refinar, compartir y visualizar sus datos.

http://www.freebase.com/[Freebase]::
Una interesante subsidiaria de Google que ofrece “un gráfico de
entidades de gente, lugares y cosas, creado por una comunidad amante de
la información abierta”.

Datos de investigación::

Hay compiladores nacionales y disciplinarios de datos de investigación
como el http://www.data-archive.ac.uk/[UK Data Archive]. Si
bien hay mucha información gratuita en el punto de acceso, también hay
muchos datos que requieren una suscripción, o que no pueden ser
reutilizados o redistribuidos sin obtener autorización.

.Obtener datos de archivos de papel
****
Justo después de la difusión por WikiLeaks de documentos militares de
EE.UU. sobre Afganistán e Irak, decidimos adaptar el concepto para
conmemorar el 50 aniversario de la Guerra de Argelia publicando los
Diarios de la Guerra de Argelia. Nos propusimos obtener y digitalizar
los archivos del Ejército Francés en Argelia. Estos están disponibles en
el archivo del ministerio de Guerra en París, aunque en formato impreso.
Enviamos a nuestros periodistas y estudiantes a tomar fotografías de los
documentos. Tratamos de escanearlos usando un scanner Canon P-150
portátil, pero no funcionó principalmente porque gran parte de los
archivos están abrochados.

Por fin se recogieron alrededor de 10000 páginas en pocas semanas. Las
pasamos por un software de reconocimiento de texto (ABBYY FineReader)
que produjo resultados pobres. Lo que es más, el ministerio
arbitrariamente negó acceso a las cajas más interesantes de archivos.
Por encima de todo, el ministerio prohíbe reeditar documentos que pueden
ser fotografiados libremente en el lugar, por lo que decidimos que no se
justificaba el riesgo y el proyecto quedó en suspenso.

&mdash; _Nicolas Kayser-Bril, Journalism++_
****

==== Pregunte en un foro

Busque respuestas existentes o haga una pregunta en 
http://getthedata.org/[Get The Data] o http://www.quora.com/[Quora]. 
GetTheData es un sitio de preguntas y respuestas 
donde puede hacer sus preguntas relacionadas
con datos, incluyendo donde encontrar datos relacionados con un asunto
particular, cómo interrogar o encontrar una determinada fuente de datos,
qué herramientas usar para explorar de modo visual, como expurgar datos,
o ponerlos en un formato con el que pueda trabajar.

==== Pregunte en una lista de correo

Las listas de correo aprovechan la sabiduría de una comunidad entera
sobre un tópico particular. Para los periodistas de datos, la
http:/bit.ly/ddj-list[Data-Driven Journalism List] y la http://bit.ly/nicar-subscribe/[NICAR-L]
son excelentes puntos de partida. Ambas listas
están pobladas de periodistas de datos y expertos en Periodismo Asistido
por Computadora (Computer-Assisted Reporting – CAR) que trabajan en todo
tipo de proyectos. Es posible que alguien haya hecho una historia como
la suya y puede tener una idea de por dónde empezar, si es que no un
vínculo directo con los datos que busca. También podría probar con
http://project-wombat.org/[Project Wombat];
(“una lista de discusión para preguntas de referencia difíciles”), las
muchas listas de correo de 
http://lists.okfn.org/mailman/listinfo[la Open Knowledge Foundation],
listas de correo en http://theinfo.org/[the Info],
o buscar listas de correo sobre el tópico o en la región que está
interesado.

==== Súmese a Hacks/Hackers

http://hackshackers.com/[Hacks/Hackers] es
una organización periodística internacional de base en rápida expansión
con docenas de secciones y miles de miembros en 4 continentes. Su misión
es crear una red de periodistas (“Hacks”) y tecnólogos (Hackers”) que
reflexionan sobre el futuro de las noticias y la información. Con una
red tan amplia, tiene grandes probabilidades de encontrar a alguien que
sepa dónde encontrar lo que busca.

==== Pregunte a un experto

Profesores, empleados públicos y gente de los distintos sectores a
menudo saben dónde buscar. Llámelos. Mándeles un correo electrónico.
Abórdelos en eventos. Aparézcase en su oficina. Pregunte amablemente.
“Estoy escribiendo una historia sobre X. ¿Dónde encuentro esto? ¿Sabe
quién tiene esto?”

==== Conozca la TI (Tecnología Informática) de los entes oficiales

A menudo ayuda entender el contexto técnico y administrativo en el que
los entes oficiales tienen su información cuando se quiere acceder a
datos. Se trate de CORDIS, COINS o THOMAS, las grandes bases de datos a
menudo resultan más útiles cuando uno conoce algo del objetivo con el
que se crearon.

Encuentre los cuadros organizativos de los entes oficiales y busque
departamentos/unidades con una función que los atraviese (por ejemplo,
informaciones, servicios TI), luego explore sus sitios en la red. Muchos
datos se archivan en distintos departamentos y mientras que para uno de
ellos la base de datos que le interesa puede ser su tesoro, otro puede
dársela sin problemas.

Busque infografías dinámicas de sitios oficiales. Estas a menudo se
basan en fuentes de datos estructurados/API que pueden ser usadas de
modo independiente (por ejemplo, aplicaciones que rastrean vuelos,
aplicaciones Java que pronostican el clima).

.Investigar registros de llamadas telefónicas
****
Hace pocos meses quise analizar los registros de llamadas telefónicas
del gobernador de Texas, Rick Perry (por entonces candidato
presidencial). Fue el resultado de un pedido, largamente esperado, de
registros públicos estaduales. Los datos vinieron esencialmente en el
formato de más de 120 páginas de documentos en calidad de fax. Era un
esfuerzo que requería ingresar datos y expurgarlos, seguido del uso de
una aplicación que permitiera buscar en la guía los titulares de los
teléfonos con los que se había comunicado el gobernador.

++++
<?dbfo-need height="1in"?>
++++

Combinando nombres con datos electorales estaduales y federales,
descubrimos que Perry tomó contacto con donantes a su campaña y con
súper comités de acción política (los llamados super PAC, que
supuestamente no deben organizar la recolección de fondos) 
http://bo.st/perry-phone[desde teléfonos de oficinas públicas estaduales], práctica mal vista
y que planteó interrogantes sobre los vínculos entre él y un “super PAC”
que trabaja para él.

&mdash; _Jack Gillum, Associated Press_
****

==== Busque nuevamente

Cuando sepa más sobre lo que está buscando, vuelva a buscar usando
frases y conjuntos de palabras improbables que descubrió desde la última
vez. ¡Quizá tenga más suerte con los motores de búsqueda!

==== Escriba un pedido de acceso a la información

Si usted cree que un ente oficial tiene los datos que necesita, un
Pedido de Acceso a Información puede ser su mejor herramienta. Vea la
siguiente sección para más información respecto de cómo presentarlo.

&mdash; _Brian Boyer (Chicago Tribune), John Keefe (WNYC), Friedrich Lindenberg (Open Knowledge Foundation), Jane Park (Creative Commons), Chrys Wu (Hacks/Hackers)_


.Cuando falla la ley
****

Luego de leer un http://bit.ly/hygiene-inspections[artículo académico]
que explica que publicar el resultado de inspecciones de higiene en
restaurantes redujo la cantidad de enfermedades relacionadas con
alimentos en Los Ángeles, pedí a los servicios de higiene parisinos la
lista de inspecciones. Siguiendo el procedimiento establecido por la ley
de Acceso a la Información francesa, esperé 30 días su negativa a
contestar, entonces fui a la Comisión de Acceso a los Datos públicos
(CADA en francés), que determina la legitimidad de los pedidos de
acceso a información. CADA apoyó mi pedido y ordenó a la administración
entregar los datos. La administración a continuación pidió dos meses más
y CADA lo aceptó. Dos meses más tarde la administración aún no había
hecho nada.

Traté de conseguir el apoyo de defensores del libre acceso a la
información famosos (y con muchos recursos) para presentar una demanda
legal (lo que hubiera costado € 5000 y se hubiera ganado sin duda con el
apoyo de CADA), pero temían complicar sus relaciones con los programas
de datos abiertos oficiales. Este ejemplo es uno entre muchos en los que
la administración francesa simplemente ignora la ley y las iniciativas
oficiales no hacen nada para apoyar pedidos de datos de periodistas
comunes.

&mdash; _Nicolas Kayser-Bril, Journalism++_
****

++++
<?dbfo-need height="2in"?>
++++

=== Su Derecho a la Información

Antes de hacer un pedido de acceso a información, debiera verificar si
los datos que está buscando ya están disponibles o si otros ya los han
pedido. El capítulo anterior tiene algunas sugerencias respecto de dónde
puede averiguar. Si ha estado mirando y aún no pudo conseguir los datos
que necesita, entonces puede querer presentar un pedido formal. Algunos
consejos que pueden ayudar a hacer más efectivo su pedido.

Planifique anticipadamente para ahorrar tiempo::
Piense en presentar un pedido formal de acceso cuando se proponga buscar
información. Es mejor no esperar hasta haber agotado todas las demás
posibilidades. Ahorrará tiempo presentado un pedido al comienzo de su
investigación y desarrollando otras investigaciones paralelamente. Esté
preparado para las demoras: a veces los entes públicos tardan en
procesar pedidos, por lo que es mejor prever esto.

Verifique las normas respecto de aranceles::
Antes de comenzar a presentar un pedido, verifique las normas respecto
de aranceles para presentar pedidos o recibir información. De ese modo,
si un funcionario público de pronto le pide dinero, sabrá cuáles son sus
derechos. Puede pedir documentos electrónicos para evitar costos de
copiado y correo, mencione en su pedido que prefiere tener la
información en formato electrónico. De ese modo evitará pagar un
arancel, a menos por supuesto que la información no esté disponible
electrónicamente, aunque en estos tiempos por lo general es posible
escanear documentos que no están digitalizados aún y luego enviarlos
como agregado por correo electrónico.

Conozca sus derechos::
Sepa cuáles son sus derechos antes de comenzar, de modo de saber donde
está parado y qué cosas están obligadas a hacer las autoridades y qué
cosas no. Por ejemplo, la mayoría de las leyes de libre acceso a
información establecen un plazo para que las autoridades respondan.
Globalmente, en la mayoría de las leyes los plazos varían de unos pocos
días a un mes. Asegúrese de conocer el plazo antes de comenzar y anote
la fecha en la que presenta su pedido.

Los entes oficiales no están obligados a procesar los datos para usted,
pero debieran darle todos los datos que tienen, y si son datos que
debieran tener para cumplir con sus obligaciones legales, por cierto que
debieran entregárselos.

Diga que conoce sus derechos::
Habitualmente no se requiere que usted mencione las leyes de acceso a
información o de libertad de información, pero esto se recomienda porque
muestra que conoce sus derechos y esto probablemente promueva una
respuesta acorde con el derecho vigente. Señalamos que en el caso de
pedidos a la UE, es importante mencionar que es un pedido de acceso a
documentos y es mejor mencionar específicamente la Norma 1049/2001.

Hágalo simple::
En todos los países es mejor comenzar con un simple pedido de
información y luego agregar más preguntas cuando obtiene la información
inicial. De ese modo no corre el riesgo de que el ente público pida
extensión del plazo por tratarse de un “pedido complejo”.

Concentre su pedido::
Un pedido de información que solo está en manos de una parte de un ente
público probablemente tenga respuesta más rápida que un pedido que
requiere una búsqueda en todo un ente. Un pedido que involucra que el
ente consulte a terceros (p.ej., una empresa privada que aportó la
información, otro gobierno que se ve afectado por la misma) puede llevar
un tiempo particularmente prolongado. Sea persistente.

Piense que hay dentro del archivo::
Intente averiguar qué datos se recogen. Por ejemplo, si recibe una copia
en blanco del formulario que llena la policía después de accidentes de
tráfico, puede ver qué información toman en cuenta y cual no respecto de
choques de autos.

Sea específico::
Antes de presentar su pedido piense: ¿es ambiguo en algún sentido? Esto
es especialmente importante si piensa comparar datos de distintos entes
públicos. Por ejemplo, si pide cifras de los _últimos 3 años_, algunos
entes le enviarán información de los últimos 3 años calendario y otros
de los 3 últimos años financieros, los que no podrá comparar
directamente. Si decide ocultar su verdadero pedido en otro más general,
entonces debe hacer su pedido lo suficientemente amplio como para que
abarque la información que quiere pero no tanto como para resultar poco
claro o como para desalentar a las autoridades a responder. Los pedidos
específicos y claros tienden a tener respuestas más celeras y mejores.

Presente múltiples pedidos::
Si no está seguro donde presentar su pedido, nada le impide presentar su
pedido a 2, 3 o más entes al mismo tiempo. En algunos casos, los varios
entes le darán distintas respuestas, pero esto en realidad le puede ser
de ayuda en cuanto a darle un cuadro más completo de la información
disponible en la materia que investiga.

Presente pedidos internacionales::
Cada vez hay más posibilidades de presentar pedidos por vía electrónica,
por lo que no importa donde vive. Alternativamente, si no vive en el
país en el que quiere presentar su pedido, puede en algunos casos enviar
el pedido a la embajada y desde allí deben transferir el pedido al ente
público competente. Tendrá que verificar en la embajada correspondiente
si están en condiciones de hacer esto: a veces el personal de la
embajada no está capacitado en la cuestión del derecho a la información
y si este parece ser el caso, es más seguro presentar le pedido
directamente al ente público correspondiente.

Haga una prueba::

Si piensa mandar el mismo pedido a muchos entes públicos, empiece por
enviar un primer texto del pedido a unos pocos entes como ejercicio
piloto. Esto le mostrará si está usando la terminología adecuada para
obtener el material que quiere y si es factible que contesten sus
preguntas, de modo de poder revisar el pedido si fuera necesario antes
de enviarlo a todos los destinatarios.

Anticipe las excepciones::

Si cree que pueden aplicarse excepciones a su pedido entonces, cuando
prepare sus preguntas, separe las preguntas relativas a información
potencialmente delicada del resto de la información que el sentido común
diría que no tiene porque ser motivo de una excepción. Luego divida sus
preguntas en 2 y presente los 2 pedidos por separado.

Pida acceso a los archivos::

Si vive cerca del lugar donde se guarda la información (por ej., en la
capital en la que se guardan los documentos), también puede pedir
inspeccionar los documentos originales. Esto puede ser de ayuda en la
investigación de información que puede estar contenida en una gran
cantidad de documentos que le gustaría ver. Tal inspección debiera ser
gratuita y debe poder realizarse en un momento que sea razonable y
conveniente para usted.

¡Guarde registro!::

Haga su pedido por escrito y guarde una copia o un archivo de modo que
en el futuro pueda demostrar que envío su pedido, en caso de tener que
apelar por falta de respuesta. Esto también le da evidencias de haber
presentado el pedido si piensa hacer un artículo sobre el tema.

Hágalo público::

Acelere las respuestas haciendo público que presentó un pedido: si
escribe o transmite la información de que se ha presentado el pedido
puede crear presión sobre la institución pública para que procese y
responda al pedido. Puede actualizar la información cuando reciba
respuesta a su pedido si pasa el plazo y no hay respuesta, puede
transformar esto en una noticia también. Hacer esto tiene el beneficio
adicional de educar al público respecto del derecho de acceso a la
información y cómo funciona en la práctica.

[NOTE]
====
También hay varios servicios excelentes que puede usar para hacer
público su pedido y toda respuesta subsecuente, poniéndolas a
disposición del público en la red, tales como 
http://www.whatdotheyknow.com/[¿Qué saben?] para entes públicos en el RU, 
https://fragdenstaat.de/[Frag den Staat] para entes públicos alemanes, 
y http://www.asktheeu.org/[Ask the EU]) para instituciones
de la UE. El proyecto http://www.alaveteli.org/[Alaveteli] está ayudando a
crear servicios similares en docenas de países en todo el mundo.
====

[[FIG043]]
.¿Qué saben? (My Society)
image::figs/incoming/04-AA.png[float="none"]

Involucre a colegas::

Si sus colegas son escépticos respecto del valor de los pedidos de
acceso a la información, una de las mejores maneras de convencerlos es
escribir un artículo basado en información que obtuvo usando una ley de
acceso a la información. También se recomienda mencionar en el artículo
final o en su alocución por radio o televisión que usó la ley, como un
modo de subrayar su valor y aumentando la conciencia del público de la
existencia de ese derecho.

Pida datos en crudo::

Si quiere analizar, explorar, o manejar datos usando una computadora,
entonces debe pedir explícitamente datos en formato electrónico que la
máquina pueda leer. Puede clarificar esto especificando, por ejemplo,
que requiere una información presupuestaria en un formato “adecuado para
su análisis con software contable”. También puede querer pedir
explícitamente la información en forma desagregada o granular. Puede
leer más acerca de esto en este informe
(http://bit.ly/access-report[http://bit.ly/access-report])

++++
<?dbfo-need height="1in"?>
++++
 
Preguntar sobre organizaciones eximidas de las leyes de acceso a la
información::

Usted puede querer investigar acerca de ONG, compañías privadas,
organizaciones religiosas y/u otras organizaciones que no están
obligadas a entregar documentación bajo las leyes de acceso a la
información. Sin embargo es posible encontrar información acerca de
ellas a través de entes públicos que sí están cubiertos por las leyes
de acceso a la información. Por ejemplo, puede preguntar a un
departamento o ministerio si han dado fondos o tratado con una compañía
privada u ONG específica y pedir documentos que respalden la
información. Si necesita más ayuda para hacer su pedido de acceso a la
información puede consultar también el http://www.legalleaks.info/toolkit.html[Legal Leaks]

&mdash; _Helen Darbishire (Access Info Europe), Djordje Padejski (Knight 
Journalism Fellow, Stanford University), Martin Rosenbaum (BBC), y 
Fabrizio Scrollini (London School of Economics and Political Science)_

.Usar pedidos de acceso a la información para entender el gasto
****
He usado pedidos de acceso a información de un par de maneras diferentes
para ayudar a cubrir COINS, la mayor base de datos de gasto, presupuesto
e información financiera del estado británico. Al comienzo de 2010
George Osborne sostuvo que si era nombrado al frente del Tesoro, daría
acceso a COINS para facilitar una mayor transparencia. En ese momento
pareció una buena idea investigar los datos y la estructura de COINS por
lo que envíe unos cuantos pedidos de acceso a la información, uno para
http://bit.ly/wdtk-coins-1[el esquema de la base de datos], otro para la
orientación que reciben los trabajadores del Tesoro cuando trabajan con
http://bit.ly/wdtk-coins-2[COINS] y un
tercero para el http://bit.ly/wdtk-coins-3[contrato del Tesoro con el proveedor de la base de datos]. 
Todo lo cual resultó en la publicación de datos útiles. También pedí todos los
códigos de gasto en la base de datos, información http://bit.ly/wdtk-coins-4[que también fue
publicada]. Todo esto ayudó a entender COINS cuando George Osborne llegó al Tesoro en
mayo de 2010 y publicó COINS en junio de 2010. Los datos de COINS fueron
usados en una cantidad de sitios de la red alentando al público a
investigar los mismos, incluyendo OpenSpending.org y el http://coins.guardian.co.uk/coins-explorer/search[Coins Data Explorer]
de The Guardian.

Luego de investigar un poco más pareció que faltaba una gran parte de la
base de datos: la Whole of Government Accounts (WGA) que son 1500
conjuntos de cuentas para entes con financiación estatal. Usé un 
http://bit.ly/wdtk-coins-5[pedido de acceso a la información para solicitar los datos WGA de 2008/09] pero no obtuve
resultados. También pedí el informe de la oficina de auditoría para WGA,
que esperaba que explicara los motivos por los que la WGA no estaba en
condiciones de publicarse. Eso también http://bit.ly/wdtk-coins-6[se me negó].

En diciembre de 2011 la WGA fue publicada en los datos COINS. Sin
embargo quería asegurarme de que hubiera suficiente orientación para
crear un conjunto completo de cuentas para cada uno de los 1500 entes
incluidos en el ejercicio de la WGA. Esto me lleva a la segunda manera
en que usé un pedido de acceso a información: para asegurarme de que los
datos difundidos bajo el plan de transparencia británico estuvieran bien
explicados y contuvieran lo que debían. Presenté un pedido de acceso a
la información http://bit.ly/wdtk-coins-7[del conjunto de cuentas para cada ente público incluido
en la WGA].

&mdash; _Lisa Evans, the Guardian_
****

===  El Wobbing* funciona. ¡Úselo!

* N. del t. Wobbing es un neologismo surgido de la jerga periodística
holandesa. La legislación de libre acceso a la información en Holanda se
conoce por la sigla WOB. De allí se deriva el término.

Usar la legislación de acceso a la información –o wobbing, como se lo
llama a veces- es una herramienta excelente pero requiere método y, a
menudo, persistencia. A continuación, 3 ejemplos de mi propio trabajo
como periodista de investigación que ilustran los puntos fuertes y los
desafíos que plantea el wobbing.

==== Estudio de caso 1: subsidios agropecuarios

Todos los años la UE paga casi € 60.000 millones a productores y el
sector agropecuario. Todos los años. Esto sucede desde fines de la
década de 1950 y el discurso político era que los subsidios ayudan a los
productores más pobres. Sin embargo, un primer logro en base a un pedido
de acceso a la información en Dinamarca en 2004 mostró que esto eran
solo palabras. Los pequeños productores estaban en graves dificultades,
de lo que a menudo se quejaban en privado y en público, y en realidad la
mayor parte de los fondos iban a unos pocos grandes terratenientes y a
la gran industria agropecuaria. De modo que obviamente quise saber:
¿Esto es un patrón que abarca a toda Europa?

En el verano de 2004 le pedí los datos a la Comisión Europea. Todos los
años en febrero la Comisión recibe datos de los estados miembros. Los
datos muestran quien solicita fondos de la UE, cuánto reciben los
beneficiarios y si lo reciben por explotar su tierra, desarrollar su
región o para exportar leche en polvo. En aquel momento la Comisión
recibía las cifras como archivos CSV en un CD. Muchos datos, pero con
los que en principio era fácil trabajar. Es decir, si uno podía
obtenerlos.

En 2004 la Comisión se negó a entregar los datos; el argumento clave fue
que los datos estaban cargados en una base de datos y recuperarlos
exigía mucho trabajo. Argumento que el Ombudsman Europeo llamó _mala
administración_. Puede encontrar todos los documentos de este caso en el
http://bit.ly/eu-wobbing[sitio sobre wobbing.eu]. Allá por 2004 no
teníamos tiempo de enredarnos en cuestiones legales. Queríamos los
datos.

[[FIG044]]
.El sitio de los subsidios agrícolas (Farmsubsidy.org)
image::figs/incoming/04-BB.png[float="0"]

Por lo que nos asociamos con gente de toda Europa para obtener los datos
país por país. Colegas ingleses, suecos y holandeses obtuvieron los
datos en 2005. Finlandia, Polonia, Portugal y regiones de España,
Eslovenia y otros países también ofrecieron su información. Incluso en
Alemania, enemiga del wobbing, logré obtener algunos datos de la
provincia del Norte del Rin – Westfalia en 2007. Tuve que recurrir a las
cortes para obtener los datos, pero resultó en algunos buenos artículos
en http://bit.ly/stern-wobbing[la revista Stern y en Stern online].

¿Fue casualidad que Dinamarca y el RU fueran los primeros en dar acceso
a sus datos? No necesariamente. Si se mira el cuadro político general,
los subsidios agropecuarios en aquel tiempo debían verse en el contexto
de las negociaciones de la OMC en las que había presión contra los
subsidios agropecuarios. Dinamarca y el RU se cuentan entre los países
más liberales de Europa, de modo que bien pudo ser que los vientos
políticos soplaran en dirección a una mayor transparencia en esos
países.

La historia no se acaba allí; para consultar más episodios y los datos,
ver http://farmsubsidy.org/[Farm Subsidy].

.Conozca sus derechos
****
Cuando publica datos, ¿debe preocuparse por el copyright y otros
derechos en los datos? Aunque debe consultar siempre con su equipo
legal, como regla: si está publicado por el estado no tiene porque pedir
perdón ni permiso; si es publicado por una organización que no gana
dinero vendiendo datos, no tiene mucho de qué preocuparse; si lo publica
una organización que obtiene ganancias con la venta de datos, entonces
decididamente tiene que pedir permiso.

&mdash; _Simon Rogers, the Guardian_
****

==== Estudio de caso 2: efectos colaterales

Todos somos conejillos de Indias en lo que se refiere a tomar
medicamentos. Las drogas pueden tener efectos secundarios. Todos sabemos
esto: sopesamos los beneficios y riesgos potenciales, y tomamos una
decisión. Desgraciadamente, esta a menudo no es una decisión basada en
información.

++++
<?dbfo-need height="1in"?>
++++

Cuando los adolescentes toman una píldora en contra de los granitos,
esperan tener piel suave, no un mal estado de ánimo. Pero esto es
precisamente lo que sucedió con una droga, con la que los jóvenes se
deprimieron y hasta tuvieron tendencias suicidas por tomarla. El peligro
de este efecto secundario en particular –-evidentemente una historia
periodística-- no era algo demasiado conocido.

Hay datos sobre efectos secundarios. Los productores tienen que entregar
información regularmente a las autoridades de salud acerca de los
efectos secundarios observados. Esa información está en manos de las
autoridades nacionales y europeas una vez que se permite la venta de la
droga.

Nuevamente en este caso se tuvo un primer logro a nivel nacional en
Dinamarca. Durante una investigación internacional de un equipo danés,
holandés y belga, Holanda también dio la información. Otro ejemplo de
salir de ronda con el _wobbing_: nos ayudó mucho poder señalar a las
autoridades holandesas que los datos estaban accesibles en Dinamarca.

Pero la historia era cierta: en Europa había gente joven con tendencias
suicidas y lamentablemente también hubo suicidios en varios países como
resultado del uso de la droga. Periodistas, investigadores y las
familias de una joven víctima presionaban duro para obtener acceso a
esta información. El Ombudsman Europeo ayudó a presionar por más
transparencia en el Ente Europeo de Medicina y http://bit.ly/eu-ombudsman[parece que tuvo éxito]. 
Por lo que ahora a los periodistas les corresponde obtener los datos y analizar el
material a fondo. ¿Somos todos conejillos de Indias, como dijo un
investigador, o son buenos los mecanismos de control?

Lecciones: no acepte una negativa cuando de lo que se trata es de
transparencia. Sea persistente y siga una historia a lo largo de los
años. Las cosas pueden cambiar, permitiendo mejor información con mejor
acceso en un momento posterior.

==== Estudio de caso 3: contrabando de muerte

La historia reciente puede ser muy dolorosa para poblaciones enteras, en
particular después de guerras y en tiempos de transición. ¿Entonces cómo
pueden obtener datos duros los periodistas para investigar, cuando –por
ejemplo- los que se beneficiaron de la última guerra ahora están en el
poder? Esta es la tarea que se propuso un equipo de periodistas
eslovenos, croatas y bosnios.

El equipo se dispuso a investigar los negocios con armas en la ex
Yugoslavia durante el embargo de la ONU a comienzos de la década de
1990. La base del trabajo fueron documentos de investigaciones
parlamentarias sobre el tema. Para documentar las rutas de embarque y
comprender la estructura del comercio, se debía rastrear el transporte
con números de embarcaciones en los puertos y las licencias de los
camiones.

Comisiones parlamentarias eslovenas han hecho investigaciones sobre las
ganancias obtenidas en las guerras de los Balcanes, pero nunca han
llegado a ninguna conclusión. Pero había un rastro extremadamente
valioso de documentos y datos desclasificados, incluyendo 6000 páginas
que el equipo esloveno obtuvo a través de un pedido de acceso a
información.

En este caso los datos debieron extraerse de documentos y bases de
datos. Al aumentar los datos con más información, análisis e
investigaciones, pudieron determinar numerosas http://bit.ly/kaasogmulvad-smuggling[rutas del comercio ilegal de armas].

El equipo tuvo éxito y los resultados son http://bit.ly/journalismfund-smuggling1[únicos]
y ya le han significado al equipo su primer premio. Lo que es más
importante, la historia es valiosa para toda la región y bien puede ser
retomada por periodistas en otros países por los que pasó la carga
mortífera.

Lecciones: publique buen material en crudo aunque lo encuentre en
lugares inesperados y combínelo con datos existentes de acceso público.

&mdash; _Brigitte Alfter, Journalismfund.eu_

.Pedidos de acceso a la información con amigos
****
Muchos países balcánicos tienen problemas de corrupción gubernamental.
La corrupción a menudo es incluso peor cuando se trata de que los
gobiernos municipales rindan cuentas en esos países. Durante varios
meses un grupo de periodistas serbios vinculados con el 
http://www.cins.org.rs/[Centre for
Investigative Reporting de Belgrado] han estado
cuestionando documentos del año 2009 de más de 30 municipalidades. Antes
de eso, casi nada estaba accesible al público. La idea era obtener los
registros originales y poner los datos en hojas de cálculo, para hacer
controles y comparaciones básicas entre las municipalidades y obtener
las cifras máximas y mínimas. Los indicadores básicos eran las cifras
presupuestarias, gastos regulares y especiales, salarios de
funcionarios, gastos de viaje, número de empleados, gastos de uso de
celular, gastos diarios, cifras de compras oficiales, y así siguiendo.
Era la primera vez que reporteros pedían esa información.

El resultado fue una base de datos que desnuda numerosas
representaciones falsas, prácticas ilegales y casos de corrupción. Una
lista de los alcaldes mejor pagos indicaba que unos cuantos de ellos
recibían más dinero que el presidente serbio. Muchos otros funcionarios
tenían sueldos excesivos, recibiendo muchos de ellos reintegros enormes
de expensas de viaje y por gastos. Los datos de gasto público obtenidos
con mucho esfuerzo ayudaron a sacar a luz un enredo oficial. De la base
de datos derivaron más de 150 artículos y muchos de ellos fueron
reeditados por los medios locales y nacionales en Serbia.

Aprendimos que comparar los registros con datos comparables de entes
gubernamentales similares puede sacar a luz desviaciones y echar luz
sobre probables hechos de corrupción. Los gastos exagerados e inusuales
solo pueden ser detectados por comparación.

&mdash; _Djordje Padejski, Knight Journalism Fellow, Stanford University_
****

=== Obtener datos de la red

Ha probado todo y no ha logrado obtener los datos que quiere. Encontró
los datos en la red pero lamentablemente no hay opciones de descarga y
fracasó en el intento de copiar y pegar. No tema, aún puede haber una
manera de obtener los datos. Por ejemplo, puede:

++++
<?dbfo-need height="1in"?>
++++

  * Obtener datos de APIs (interfaces de programación de aplicaciones)
online, tales como interfaces provistas por bases de datos y muchas
aplicaciones modernas (incluyendo Twitter, Facebook y otras). Esta es
una manera fantástica de acceder a datos oficiales o comerciales, así
como datos de redes sociales.
  * Extraer datos de PDF. Esto es muy difícil, dado que PDF es un lenguaje
para impresoras y no retiene mucha información sobre la estructura de
los datos presentados en el documento. Extraer información de PDF va más
allá del alcance de este libro, pero hay algunas herramientas y
tutoriales que pueden ayudarlo a hacerlo.
  * Usar "screen scraping" para obtener datos de sitios de la red. Se trata
de extraer contenido estructurado de una página normal de la red con la
ayuda de un programa de de recuperación de información o escribiendo una
pequeña pieza de software. Si bien este método es muy poderoso y puede
ser usado en muchos lugares, requiere comprender un poco cómo funciona
la red.

Con todas esas opciones técnicas, no olvide las opciones simples: a
menudo vale la pena invertir un poco de tiempo en buscar un archivo con
datos que pueden ser interpretados por una computadora o llamar a la
institución que tiene los datos que usted quiere.

En este capítulo presentamos un ejemplo muy básico de _scrapear_ datos
de una página HTML.

==== ¿Qué son los datos procesables por computadora?

Para la mayoría de estos métodos, el objetivo es obtener acceso a datos
que puedan ser nterpretados por una computadora. Tales datos son creados
para ser procesados por una computadora en vez de ser presentados a un
usuario humano. La estructura de estos datos se relaciona con la
información contenida en ellos, y no la manera en que será presentada
eventualmente. Entre los ejemplos de formatos que son fáciles de
interpretar por una computadora se incluyen CSV, XML, JSON, y los
archivos Excel, mientras que formatos como los de documentos Word,
páginas HTML, y archivos PDF están más relacionados con la presentación
visual de la información. Por ejemplo, PDF es un lenguaje que le habla
directamente a su impresora; le interesa la posición de líneas y puntos
en una página, en vez de caracteres distinguibles.

===="Scrapear" sitios de la red: ¿Para qué?

Todos lo han hecho: se va a un sitio de la red, uno ve una tabla
interesante y trata de copiarla a Excel de modo de poder agregar algunas
cifras o guardarla para después. Pero a menudo esto no funciona
realmente, o la información que quiere está desparramada en una gran
cantidad de sitios. Copiar a mano se puede volver rápidamente muy
tedioso, por lo que tiene sentido usar un poco de código para hacerlo.

La ventaja del "scraping" es que se puede hacer prácticamente con
cualquier sitio, desde el pronóstico del tiempo hasta el gasto
gubernamental, incluso si el sitio no tiene una API para acceso a los
datos en crudo.

==== Lo que se puede y lo que no se puede "scrapear"

Por supuesto, hay límites a lo que se puede_scrapear_. Entre los
factores que dificultan _scrapear_ en un sitio se incluyen:

  * Código HTML mal formateado con poco o nada de información estructural
(por ejemplo, sitios oficiales más antiguos).
  * Los sistemas de autenticación que se supone impiden el acceso
automático (códigos CAPTCHA y exigencia de suscripción paga).
  * Sistemas basados en sesiones que usan cookies de navegador para
rastrear lo que hace el usuario.
  * Falta de listados completos de ítems y ausencia de posibilidades de
búsquedas con comodines.
  * Bloqueado de acceso por administradores de servidores.


Otro conjunto de limitaciones son las barreras legales: algunos países
reconocen los derechos de bases de datos, lo que puede limitar su
derecho a reutilizar información que ha sido publicada online. A veces
se puede ignorar la licencia y usarla de todos modos, dependiendo de su
jurisdicción, puede tener derechos especiales como periodista. No
debería haber problema en "scrapear" datos del estado de libre
disponibilidad, pero quizás sea mejor cerciorarse antes de publicarlos.
Organizaciones comerciales -–y ciertas ONGs-- reaccionan con menos
tolerancia y pueden tratar de sostener que usted está “saboteando” sus
sistemas. Otras informaciones pueden violar la privacidad de individuos,
y por tanto, violar las leyes de privacidad de datos o la ética
profesional.

.Emparchar, "Scrapear", compilar, limpiar
****
El desafío con muchos datos británicos no es lograr obtenerlos, si no
ponerlos en un formato que se pueda usar. Se publican muchos datos sobre
hospitalidad, los intereses de los parlamentarios fuera de su función
pública, lobbys, y más como cosa habitual, pero en formatos difíciles de
analizar.

Para algunos datos, la única alternativa es el trabajo duro: unir
docenas de archivos Excel, cada uno conteniendo solo una docena de
registros, fue la única manera de hacer listas completas de reuniones
ministeriales. Para otros datos, "scrapear" la red se demostró
increíblemente útil.

Usar un servicio como ScraperWiki para pedir a programadores que
produzcan un _scraper_ que permita reunir información como el Registro
de intereses de parlamentarios, nos ahorró la mitad del trabajo: tuvimos
toda la información de los parlamentarios en una hoja, lista para la
“larga” tarea de analizarla y expurgarla.

Servicios como éste (o herramientas tales como Outwit Hub) son de
inmensa ayuda para periodistas que tratan de compilar datos complicados
y que son capaces de programar.

&mdash; _James Ball, the Guardian_
****

==== Herramientas que lo ayudan a "scrapear"

Hay muchos programas que pueden ser usados para extraer información en
masa de un sitio, incluyendo extensiones de navegadores y algunos
servicios de la red. Según el navegador que use, herramientas como
http://www.readability.com/[Readability],
que ayuda a extraer texto de una página o 
http://www.downthemall.net/[DownThemAll], que le
permite descargar muchos archivos al mismo tiempo), le ayudarán a
automatizar algunas tareas tediosas, mientras que la http://bit.ly/chrome-scraper[extensión Scraper
de Chrome] fue creada explícitamente para extraer tablas de sitios de la red.
Extensiones para programadores como http://getfirebug.com/[FireBug] para Firefox, lo mismo
ya viene incluido en Chrome, Safari e IE) le permite ver exactamente
como está estructurado un sitio y qué comunicaciones se dan entre su
navegador y el servidor.

ScraperWiki es un sitio que le permite crear _scrapers_ en una
cantidad de lenguajes de programación diferentes., incluyendo Python,
Ruby y PHP. Si quiere comenzar a _scrapear_ sin la complicación de
instalar una plataforma de programación en su computadora esta es la
manera de hacerlo. Otros servicios de la red, tales como las Hojas de
Cálculo de Google y Yahoo! Pipes, también permiten realizar extracciones
de otros sitios.

==== ¿Cómo funciona un "Scraper" de la red?

Los "scrapers" de la red por lo general son piezas pequeñas de código
escritas en un lenguaje de programación tal como Python, Ruby o PHP.
Escoger el lenguaje adecuado depende en gran medida de a qué comunidad
tiene acceso: si en su redacción o ciudad hay alguien que ya trabaja con
uno de estos lenguajes, entonces tiene sentido adoptar el mismo
lenguaje.

Si bien algunas de las herramientas de "scraping" con las que basta
cliquear y apuntar mencionadas más arriba pueden ser de ayuda para
comenzar, lo verdaderamente complejo a la hora de _scrapear_ en un sitio
es encontrar las páginas indicadas y los elementos indicados dentro de
estas páginas para extraer la información deseada. Estas tareas no
tienen que ver con programación, sino con comprender la estructura del
sitio y la base de datos.

Al presentar un sitio, su navegador casi siempre usará dos tecnologías,
HTTP, para comunicarse con el servidor y pedir recursos específicos,
tales como documentos, imágenes o videos; y HTML, el lenguaje en el que
se componen los sitios.


==== La anatomía de una página de la red

Toda página HTML está estructurada como una jerarquía de módulos (que
están definidos por etiquetas de HTML). Un módulo grande contiene muchos
módulos más pequeños –por ejemplo una tabla que tiene muchas divisiones
más pequeñas: filas y celdas. Hay muchos tipos de etiquetas que realizan
distintas funciones: algunas producen módulos, otras tablas, imágenes o
vínculos. Las etiquetas también pueden tener propiedades adicionales
(por ejemplo, pueden ser identificadores únicos y pueden pertenecer a
grupos llamados “clases” que hacen posible apuntar a y capturar
elementos individuales dentro de un documento). Escoger elementos
apropiados de esta manera y extraer su contenido es la clave para
escribir un "scraper".

Viendo los elementos en una página de la red, todo puede dividirse en
módulos dentro de módulos.

Para "scrapear" en páginas de la red tendrá que aprender un poco acerca
de los distintos tipos de elementos que pueden encontrarse en un
documento HTML. Por ejemplo, el elemento <table> abarca toda una tabla,
que tiene <tr> (table row) elementos para sus filas, que a su vez
contienen <td> (table data) para cada celda. El tipo de elemento más
común que encontrará es <div>, que puede significar básicamente
cualquier bloque de contenido. La manera más fácil de conocer estos
elementos es usar la barra de desarrolladores, http://bit.ly/developer-toolbar[developer toolbar], de su
navegador: le permitirá posicionarse sobre cualquier parte de una página
de la red y ver el código correspondiente.

Las etiquetas funcionan como el comienzo y el fin de un libro, marcando
el comienzo y el fin de una unidad. Por ejemplo +<em>+ _significa el
comienzo de un tramo de texto en itálica o destacado y_ +</em>+ significa
el fin de ese tramo. Fácil.

==== Un ejemplo: "Scraping" de incidentes nucleares con Python

http://www-news.iaea.org/EventList.aspx[NEWS] es el portal de la Agencia Internacional de Energía Atómica (AIEA) que
sigue los incidentes de radiación en todo el mundo (y disputa el título
máximo del club de los títulos raros). La página tiene listas de
incidentes en un sitio simple, tipo blog, que puede ser fácilmente "scrapeado".

[[FIG045]]
.El portal de la Agencia Internacinal de Energía Atómica (IAEA) (news.iaea.org)
image::figs/incoming/04-CC.png[float="none"]

Para empezar, cree un nuevo scraper Python en ScraperWiki y se le
presentará un área para texto mayormente vacía, salvo por algo de código
de soporte. En otra pestaña del navegador abra el http://www-news.iaea.org/EventList.aspx[sitio de AIEA 
y abra la barra para desarrolladores de su navegador. En la vista de “elementos”
trate de encontrar el elemento HTML de uno de los títulos de noticias.
La barra para desarrolladores de su navegador le ayudará a relacionar
los elementos en la página con el código HTML correspondiente.

Al investigar esta página se revelará que los títulos son elementos +<h4>+
dentro de una <table>. Cada evento es una fila +<tr>+, que también
contiene una descripción y una fecha. Si queremos extraer los títulos de
todos los eventos, debiéramos buscar la manera de seleccionar cada fila
en la tabla secuencialmente, recopilando todo el texto dentro de los
elementos de título.

Para convertir este proceso en código, tenemos que tomar conciencia de
todos los pasos. Para tener idea del tipo de pasos requeridos, juguemos
un juego simple: en su ventana de ScraperWiki trate de escribir
instrucciones individuales para cada cosa que va a hacer mientras
escribe este "scraper", como los pasos de una receta (ponga al comienzo
de cada línea un signo de numeral para indicarle a Python que no es un
verdadero código de computación). Por ejemplo:

----
# Buscar todas las filas en la tabla
# Unicornio no debe desbordar el lado izquierdo.
----
 
Trate de ser lo más preciso posible y no suponga que el programa sabe
algo acerca de la página que intenta _scrapear_.

Una vez que haya escrito algo de este seudo-código, comparemos esto con
el código esencial para nuestro primer _scraper_:

----
import scraperwiki
from lxml import html
----

En esta primera sección estaba importando funcionalidad existente de
bibliotecas, recortes de código ya escrito. +Scraperwiki+ nos dará la
capacidad de descargar sitios de la red, mientras que +lxml+ es una
herramienta para el análisis estructurado de documentos HTML. Buena
noticia: si está escribiendo un scraper con ScraperWiki, estas dos
líneas siempre serán las mismas.

----
url = "http://www-news.iaea.org/EventList.aspx"
doc_text = scraperwiki.scrape(url)
doc = html.fromstring(doc_text)
----

A continuación el código hace un nombre (variable): url, y asigna el URL
de la página de la AIEA como su valor. Esto le dice al "scraper" que
esta cosa existe y que queremos prestarle atención. Nótese que el URL
mismo está entre comillas ya que no es parte del código del programa
sino un _string_, una secuencia de caracteres.

Entonces usamos la variable del url como entrada de una función,
+scraperwiki.scrape+. Una función que producirá una tarea definida, en
este caso, descargará una página de la red. Cuando termine, asignará su
producto a otra variable, +doc_text+. +doc_text+ ahora contendrá el texto
del sitio; no la forma visual que ve en su navegador, sino el código
fuente, incluyendo todas las etiquetas. Dado que esta forma no es muy
fácil de analizar, usaremos otra función, +html.fromstring+, para
generar una representación especial, en la que podamos fácilmente
referirnos a elementos, el así llamado modelo de documento de objeto o
document object model (DOM).

----
for row in doc.cssselect("#tblEvents tr"):
link_in_header = row.cssselect("h4 a").pop()
event_title = link_in_header.text
print event_title
----

En este paso final, usamos el DOM para encontrar cada fila de nuestra
tabla y extraer el título del evento de su encabezado. Se usan dos
conceptos nuevos: el riso "for loop" y selección de elemento o
"element selection" (+.cssselect+). El "for loop" hace esencialmente lo que
implica su nombre; atraviesa una lista de ítems, asignando a cada uno un
alias temporal (+row+ en este caso) y luego aplicará las instrucciones
con sangría para cada ítem.

El otro concepto nuevo, selección de elemento o "element selection", es hacer uso de un lenguaje
especial para encontrar elementos en el documento. Los selectores CSS
son usados normalmente para agregar información de diseño a elementos
HTML y puede ser usado para extraer con precisión un elemento de una
página. En este caso (línea 6) estamos seleccionando #tb1Events tr, que
hará corresponder cada +<tr>+ en el elemento tabla con el ID tb1Events (el
signo numeral simplemente significa ID). Nótese que esto producirá una
lista de elementos +<tr>+.

Eso puede verse en la siguiente línea (línea 7i), donde estamos
aplicando otro selector para encontrar cualquier +<a>+ (que es un
hipervínculo) dentro de un +<h4>+ (un título). Aquí sólo queremos ver un
elemento (solo hay un título por fila), de modo que tenemos que sacarlo
del encabezado de la lista creada por nuestro selector con la función
+.pop()+.

Nótese que algunos elementos en el DOM contienen texto (es decir, aneder
usando la sintaxis +[element].text+ que se ve en la línea 8. Finalmente en
la línea 9 estamos imprimiendo ese texto a la consola ScraperWiki. Si
hace clic en "run" en su "scraper", la ventana más pequeña ahora debiera
comenzar a listar los nombres del evento del sitio de la AIEA.

[[FIG046]]
.Un scraper en acción (ScraperWiki)
image::figs/incoming/04-DD.png[scale="90",float="none"]

++++
<?dbfo-need height="1in"?>
++++

Ahora puede ver un "scraper" básico operando: descarga la página, la
transforma a la forma DOM, y luego le permite seleccionar y extraer
cierto contenido. Dado este esqueleto, puede tratar de resolver algunos
de los problemas que quedan usando la documentación del ScraperWiki y
Python:

  * ¿Puede encontrar la dirección del vínculo en el título de cada evento?
  * ¿Puede seleccionar el pequeño módulo que contiene la fecha y el lugar
usando su nombre de clase CSS y extraer el texto del elemento?
  * ScraperWiki ofrece una pequeña base de datos para cada scraper, de
modo que pueda almacenar los resultados; copie el ejemplo
correspondiente de sus docs y adáptelo de modo que guarde los títulos,
vínculos y fechas del evento.
  * La lista de eventos tiene muchas páginas; ¿puede _scrapear_ múltiples
páginas para obtener eventos históricos también?

Mientras intenta resolver estos desafíos, investigue un poco el
ScraperWiki: hay muchos ejemplos útiles en los "scrapers" existentes;
a menudo los datos son bastante interesantes también. De este modo no
necesita comenzar su "scraper" de cero: simplemente escoja uno similar,
tómelo y adáptelo a su problema.

&mdash; _Friedrich Lindenberg, Open Knowledge Foundation_

."Scrapear" en una base de datos pública
****
Algunos médicos franceses pueden establecer libremente sus honorarios,
por lo que uno puede pagar entre € 70 y € 500 por una consulta de 30
minutos con un oncólogo, por ejemplo. Los datos sobre honorarios por ley
son públicos, pero la administración solo ofrece una base de datos
online difícil de navegar. Para tener una buena visión de los honorarios
de los médicos para Le Monde, decidí "scrapear" toda la base de datos.

Ahí comenzó la diversión. De entrada, el formulario de búsqueda era una
aplicación Flash que redirigía a una página de resultados HTML vía un
pedido POST. Con ayuda de Nicolas Kayser-Bril, nos llevó algo de tiempo
descubrir que la aplicación usaba una tercera página como paso “oculto”
entre el formulario de búsqueda y la página de resultado. Esta página se
usaba en realidad para almacenar un cookie con valores del formulario de
búsqueda al que entonces accedía la página de resultados. Hubiese sido
difícil imaginarse un proceso más enredado, pero las opciones de la
biblioteca cURL en PHP permiten superar fácilmente las vallas, una vez
que se sabe cuáles son. Finalmente apoderarnos de la base de datos llevó
10 horas, pero valió la pena.

&mdash; _Alexandre Léchenet, Le Monde_
****

++++
<?dbfo-need height="1in"?>
++++

=== La red como fuente de datos ===

¿Cómo puede saber más de algo que solo existe en Internet? Esté buscando
una dirección de correo electrónico, sitio, imagen o artículo de
Wikipedia, en este capítulo haré con usted una recorrida por las
herramientas que le dirán más sobre ellos.

=== Herramientas web

Primero, unos cuantos servicios diferentes que puede usar para descubrir
algo más sobre todo un sitio, en vez de una página particular:

Whois::

Si va a http://whois.domaintools.com/[whois.domaintools.com/] o
simplemente tipea whois seguido de un URL _www.ejemplo.com_  en Terminal.app en una Mac
puede obtener la información básica de registro de cualquier sitio. En
los últimos años algunos dueños han preferido el registro privado, lo
que oculta sus detalles, pero en muchos casos verá un nombre, dirección,
correo electrónico y número de teléfono de la persona que registró el
sitio. También puede ingresar direcciones IP numéricas aquí y obtener
datos sobre la organización o el individuo que es dueño del servidor.
Esto es especialmente útil cuando trata de encontrar más información
sobre un usuario abusivo o malicioso de un servicio, ya que la mayoría
de los sitios registran una dirección IP de todo el que accede a ellos.

Blekko::
  El motor de búsquedas (http://blekko.com/[Blekko]
ofrece una cantidad inusual de información sobre las estadísticas
internas que reúne sobre sitios mientras recorre la red. Si tipea un
nombre de dominio seguido de “/seo”, verá una página de información
sobre ese URL. 
  La primera pestaña en <<FIG048>> le muestra qué otros
sitios se vinculan con el dominio por orden de popularidad. Esto puede
ser extremadamente útil cuando está tratando de comprender qué tipo de
cobertura recibe un sitio y por qué tiene un alto ranking en los
resultados de búsquedas de Google, ya que estos se basan en esos
vínculos entrantes. <<FIG049>> le dice qué otros sitios funcionan en
la misma máquina. Es común que estafadores y la gente que envía spam se
trate de legitimar construyendo múltiples sitios que se ensalzan y
vinculan mutuamente. Parecen dominios independientes e incluso pueden
tener detalles de registro diferentes, pero a menudo están en el mismo
servidor porque eso es mucho más barato. Estas estadísticas le dan una
visión de la estructura oculta del sitio que investiga.

[[FIG047]]
.El buscador Blekko Blekko.com)
image::figs/incoming/06-PP-01.png[float="none"]

[[FIG048]]
.Comprender la popularidad en la red, ¿quién se vincula con quién? La otra pestaña útil es “Estadísticas de Navegación"), especialmente la sección “Co-huesped con”.(Blekko.com)
image::figs/incoming/06-PP-02.png[float="none"]

[[FIG049]]
.Descubrir spammers y estafadores de la red (Blekko.com)
image::figs/incoming/06-PP-03.png[float="none"]

Compete.com::
Al estudiar una muestra representativa de consumidores estadounidenses,
http://ww.compete.com/[Compete.com] acumula estadísticas
de uso detalladas para la mayoría de los sitios y pone a disposición
gratuitamente algunos detalles básicos. Elija la pestaña de Site Profile
(Perfil de Sitio) e ingrese un dominio (<<FIG0410>>). Entonces verá un
gráfico del tráfico del sitio en el último año, junto con cifras de
cuánta gente lo visitó y con qué frecuencia (como en <<FIG0411>>).
Dado que se basan en muestras los números son solo aproximados, pero yo
los encontré razonablemente precisos cuando pude compararlos con la
analítica interna. En particular, parecen ser una buena fuente para
comparar dos sitios, dado que aunque las cifras absolutas pueden ser
equivocadas para ambos, de todos modos es una buena representación de su
diferencia relativa en cuanto a popularidad. Pero solo estudian a los
consumidores estadounidenses, por lo que los datos serán pobres para los
sitios predominantemente internacionales.

[[FIG0410]]
.El servicio de perfil de Compete (Compete.com)
image::figs/incoming/06-PP-04.png[float="none"]

[[FIG0411]]
.¿Qué está de moda? ¿De qué hay demanda?: Lugares calientes de la red (Compete.com)
image::figs/incoming/06-PP-05.png[float="none"]

 El buscador de sitios (Site Search) de Google::
   Un recurso que puede ser extremadamente útil cuando trata de explorar
todo el contenido de un dominio particular es ingresar en el buscador
los términos “sitio”: palabra clave. Si agrega “site:ejemplo.com” a su
frase de búsqueda, Google solo presentará resultados del sitio que ha
especificado. Incluso puede afinar aún más la búsqueda incluyendo el
prefijo de las páginas que le interesan, por ejemplo, “site:
ejemplo.com/páginas/”, y solo verá los resultados que responden a ese
patrón. Esto puede ser extremadamente útil cuando trata de encontrar
información que los dueños de dominios ofrecen públicamente pero que no
desean difundir, de modo que elegir las palabras claves correctas puede
permitir descubrir material muy revelador.

==== Páginas, imágenes y videos en la red

A veces lo que interesa es la actividad que rodea una historia
específica, en vez de un sitio entero. Las herramientas que se presentan
a continuación le dan distintos ángulos de cómo lee, responde, copia y
comparte contenido la gente en la red.

Bit.ly::
  Siempre recurro a http://bitly.com/[bitly.com] cuando
quiero saber cómo comparte la gente un vínculo particular. Para usarlo,
ingrese el URL que le interesa. Luego haga clic en el vínculo Info
Page+. Eso lo lleva a la página de estadísticas completas (aunque puede
tener que escoger el vínculo “aggregate bit.ly” primero si ha ingresado
en el servicio). Esto le dará una idea de la popularidad de la página,
incluyendo actividad en Facebook y Twitter y debajo de eso verá
conversaciones públicas respecto del vínculo provistas por backtype.com.
Esta combinación de datos de tráfico y conversaciones me resulta muy
útil cuando trato de comprender por qué un sitio o página es popular y
quiénes son sus fans. Por ejemplo me aportó fuertes evidencias de que la
opinión dominante respecto de la relación de Sarah Palin con los
delegados de base era equivocada.

++++
<?dbfo-need height="1in"?>
++++


Twitter::
  Al ser el servicio de micro-blogging más usado, es útil parar ver en qué
medida la gente comparte y habla acerca de piezas de contenido
individuales. Es engañosamente simple descubrir conversaciones públicas
sobre un vínculo. Uno simplemente pega el URL en el que está interesado
en la ventana de búsqueda y luego posiblemente hace clic en “más tweets”
para ver todos los resultados.

Cache de Google::
  Cuando una página se vuelve polémica los editores la pueden bajar o
alterarla sin reconocerlo. Si cree que se está encontrando con este
problema, el primer lugar a ir es el cache de Google de la página tal
como era cuando hizo su último recorrido. La frecuencia de los
recorridos está aumentando constantemente, por lo que tendrá más suerte
si intenta esto dentro de las pocas horas posteriores a que se
produjeron los supuestos cambios. Ingrese el URL correspondiente en la
ventana de búsqueda de Google y luego haga clic en la flecha triple, a
la derecha del resultado para esa página. Debiera aparecer una vista
gráfica y si tiene suerte habrá un pequeño vínculo de “Cache” arriba.
Haga clic allí para ver la toma de Google de la página. Si hay problemas
para que cargue, puede cambiar a la página más primitiva, solo de texto,
haciendo clic en otro link arriba de la página en cache completa. Usted
tendrá que guardar la imagen de la pantalla o copiar y pegar el
contenido significativo que encuentre, dado que puede quedar invalidado
en cualquier momento por nuevos cambios.

La Wayback Machine (Máquina de Hace Tiempo) del Archivo de Internet::
  Si necesita saber cómo ha cambiado una página particular en un período
de tiempo más largo, como meses o años, el Archivo de Internet tiene un
servicio llamado http://archive.org/web/web.php[The Wayback Machine] que
periódicamente hace tomas de las páginas más populares de la red. Vaya
al sitio, ingresa el vínculo que quiere buscar y si hay copias, le
mostrará un calendario para el momento que quiere examinar. Entonces
presentará una versión de la página aproximadamente como era en aquel
momento. A menudo le faltará diseño o imágenes, pero por lo general
basta para entender cuál era el foco del contenido de la página en ese
momento.

Ver el Código Fuente::
  Es algo un poco improbable, pero los diseñadores a menudo dejan
comentarios u otros indicios en el código HTML de cualquier página.
Estará en distintos menúes según el navegador que use, pero siempre hay
una opción de “view source” (ver código fuente), que le permitirá
recorrer el HTML en crudo. No necesita entender lo que significan las
partes solo legibles para la máquina, solo esté atento a los tramos de
texto que a menudo están desparramados en medio del código. Aunque solo
sean referencias de copyright o menciones de los nombres del autor,
estos a menudo pueden dar pistas importantes acerca de la creación y el
objetivo de la página.

TinEye::
  A veces uno realmente quiere conocer el origen de una imagen, pero sin
un texto claro que lo indique no hay ninguna manera evidente de hacerlo
con motores de búsqueda tradicionales como Google. 
http://www.tineye.com/[TinEye] ofrece un proceso
especializado de “búsqueda inversa de imagen”, donde uno le da la imagen
que tiene y encuentra otras imágenes en la red que se ven muy similares.
Debido a que usa reconocimiento de imagen para hacer la búsqueda,
funciona incluso cuando una copia ha sido recortada, distorsionada o
comprimida. Esto puede ser extremadamente efectivo cuando usted sospecha
que una imagen que se presenta como original o nueva no lo es, dado que
puede reconducirlo a la verdadera fuente original.

YouTube::
  Si hace clic en el ícono de estadísticas en el ángulo inferior derecho
de cualquier video, puede conseguir información valiosa sobre su público
a lo largo del tiempo. Si bien no es completa, es útil para entender
aproximadamente quienes son los espectadores, de donde vienen y cuándo.

==== Correo electrónico
Si está investigando correos electrónicos, a menudo querrá conocer más
detalles sobre la identidad y ubicación del que los envió. No hay una
buena herramienta disponible para ayudar con esto, pero puede ser muy
útil conocer lo básico acerca de los encabezados ocultos incluidos en
todo mensaje de correo electrónico. Estos funcionan como indicadores
para el correo y pueden revelar mucho acerca del remitente. En
particular, a menudo incluyen la dirección IP de la máquina desde la que
fue enviado el correo, parecido a la identidad del que hace una llamada
telefónica. Puede entonces usar "whois" con ese número IP para saber qué
organización posee esa máquina. Si resulta ser alguien como Comcast o
AT&T que proveen conexiones a consumidores, entonces puede visitar
MaxMind para obtener su ubicación aproximada.

Para ver estos encabezados en Gmail abra el mensaje y
[line-through]*abra*el menú junto a la respuesta arriba a la derecha y
elija “Mostrar original”.

Entonces verá una nueva página que revela el contenido oculto. Al
comienzo habrá un par de docenas de líneas que son palabras seguidas por
una coma. La dirección IP que busca puede estar allí, pero el nombre
dependerá de cómo fue enviado el correo. Si se envió desde Hotmail, se
llamará +X-Originating-IP:+, pero si fue enviado desde Outlook o Yahoo
estará en la primera línea que comienza con +Received:+.

Si investigo la dirección con Whois me dice que está asignado a Virgin
Media, un ISP del RU, por lo que uso el servicio de ubicación geográfica
de MaxMind para descubrir que viene de mi ciudad, Cambridge. Esto
significa que puedo estar razonablemente confiado de que se trata
efectivamente de un correo de mis padres y no de impostores.

==== Tendencias

Si está investigando un tema amplio en vez de un sitio o ítem
particular, estas son algunas herramientas que pueden ayudar:

Wikipedia Article Traffic (Tráfico de Artículos de Wikipedia)::
  Si le interesa conocer cómo ha variado el interés del público sobre un
tema o persona a lo largo del tiempo, puede encontrar cifras de vistas
día por día para cualquiera página de Wikipedia en http://stats.grok.se/[stats.grok.se].
Es un sitio un poco tosco, pero le permitirá
descubrir la información que necesita revolviendo un poco. Ingrese el
nombre que le interesa para tener una visión mensual del tráfico en esa
página. Eso le presentará un gráfico que muestra cuántas veces fue vista
la página cada día del mes que usted especifique. Desgraciadamente solo
se puede ver un mes por vez, por lo que tendrá que seleccionar otro mes
y volver a buscar, para ver cambios en períodos más prolongados.

Google Insights::
  Puede tener una clara visión de los hábitos de búsquedas del público
usando http://www.google.com/insights/search/[Insights de Google] (<<FIG0412>>).
Ingrese un par de frases de búsquedas comunes, como “Justin Bieber vs Lady Gaga”, 
y verá un gráfico de sus números relativos de búsquedas con el paso del tiempo. 
Hay muchas opciones para refinar su vista de los datos, desde zonas geográficas más reducidas hasta más
detalle a medida que pasa el tiempo. Lo único que falta son valores
absolutos: solo verá porcentajes relativos, lo que puede ser difícil de
interpretar.

[[FIG0412]]
.Google Insights (Google) 
image::figs/incoming/06-PP-06.png[float="none"]

&mdash; _Pete Warden, analista de datos y diseñador independiente_

=== Crowdsourcing en el Datablog de The Guardian

"Crowdsourcing", http://es.wikipedia.org/wiki/Crowdsourcing[según Wikipedia],
“consiste en externalizar tareas que, tradicionalmente, realizaba un
empleado o contratista, a un grupo numeroso de personas o una comunidad,
a través de una convocatoria abierta”. Lo que sigue está tomado de una
entrevista con Simon Rogers acerca de cómo el Datablog usó
"crowdsourcing" para cubrir el escándalo de los gastos de
parlamentarios, el uso de drogas y los papeles de Sarah Palin:

A veces uno recibe una tonelada de archivos, estadísticas o informes que
es imposible que una persona pueda analizar. También puede conseguir
material que es inaccesible o está en un mal formato y no puede hacer
demasiado. Es en esto que puede ayudar el "crowdsourcing".

Una cosa que tiene The Guardian es muchos lectores, muchos pares de
ojos. Si hay un proyecto interesante en el que necesitamos su ayuda,
entonces se lo pedimos. Es lo que hicimos con los http://mps-expenses.guardian.co.uk/[Gastos de los parlamentarios].
Teníamos 450.000 documentos y poco tiempo para hacer algo. ¿Entonces qué
cosa mejor que repartir la tarea entre los lectores?

[[FIG0413]]
.Una copia redactada de los gastos incidentales de Stephen Pound (The Guardian)
image::figs/incoming/04-EE.png[float="none"]

El proyecto de los gastos de los parlamentarios generó muchas pistas.
Tuvimos más historias que datos. El proyecto fue llamativamente exitoso
en términos de tráfico. A la gente realmente le gustó.

Actualmente estamos http://bit.ly/guardian-drugs[haciendo algo con MixMag sobre el uso de drogas], 
que también ha sido fenomenal. Parece que va a ser más grande que la
encuesta sobre crímenes en Gran Bretaña en términos de la cantidad de
gente que vuelve, lo que es brillante.

Lo que ambos proyectos tienen en común es que se refieren a temas que
realmente le importan a la gente, por lo que está dispuesta a dedicarles
su tiempo. Mucho del _crowdsourcing_ que hemos hecho depende de la ayuda
de obsesivos. Con los gastos de los parlamentarios tuvimos una cantidad
masiva de tráfico al comienzo y luego bajó. Pero seguimos teniendo gente
que lee obsesivamente cada página buscando anomalías e historias. Una
persona ha leído 30.000 páginas. Saben muchas cosas.

También usamos "crowdsourcing" con http://bit.ly/guardian-palin-papers[los papeles de Sarah Palin].
También en este caso fue de gran ayuda para estudiar la información en
crudo en busca de historias.

En términos de generar historias el "crowdsourcing" ha funcionado muy
bien. A la gente realmente le gusta e hizo quedar bien a The Guardian.
Pero en términos de generar datos no hemos usado el "crowdsourcing"
tanto.

Algunos de los proyectos de "crowdsourcing2 que hemos hecho y que
funcionaron realmente bien, han sido encuestas a la antigua. Cuando uno
le pregunta a la gente acerca de su experiencia, su vida, lo que han
hecho, eso funciona muy bien porque la gente no tiende a inventar en
esos casos. Dice lo que siente. Cuando le pedimos a la gente que haga
nuestro trabajo por nosotros hay que encontrar una especie de marco para
que la gente produzca datos de un modo que resulten confiables.

Respecto de la confiabilidad de los datos, creo que 
la postura de http://www.oldweather.org/[Old Weather] es realmente buena. Consiguen que
10 personas hagan cada entrada, que es una buena manera de asegurarse
precisión. Con los gastos de los parlamentarios tratamos de minimizar el
riesgo de que los mismos parlamentarios se metieran online a editar sus
datos para quedar mejor. Pero no se puede estar permanentemente
cuidándose de esto. Sólo se puede estar atento a ciertos URL o si
provienen de la zona SW1 de Londres. Así que eso es un poco más difícil.
Los datos que sacábamos no eran siempre confiables. Aunque las historias
eran muy buenas, no producía números en crudo que pudiéramos usar con
certeza.

Si tuviera que dar consejos a quienes aspiran a ser periodistas de datos
y que quieren usar el "crowdsourcing" para obtener datos, los alentaría a
hacerlo con algo que a la gente realmente le importa y que le seguirá
importando cuando deje de producir titulares de primera página. Además,
si uno puede crear algo que se parezca a un juego, eso puede ayudar
realmente a atraer a la gente. Cuando hicimos la historia de los gastos
por segunda vez, fue mucho más como un juego con tareas individuales
para que las hiciera la gente. Realmente fue de ayuda dar a la gente
tareas específicas. Eso fue importante porque creo que si uno solo le
presenta a la gente una montaña de información que tiene que ver y le
dice “mire esto”, puede resultar un trabajo duro y poco grato. Por lo
que creo que es realmente importante hacer que sea divertido.

&mdash; _Marianne Bouchart, Data Journalism Blog, interviewing Simon Rogers, the Guardian_

=== Cómo el Datablog usó "crowdsourcing" para cubrir la venta de entradas para las Olimpíadas

Creo que el proyecto de _croudsourcing_ que tuvo la mayor respuesta fue
un http://bit.ly/guardian-olympics[trabajo sobre la subasta de entradas para las Olimpíadas].
Miles de personas en el RU trataron de obtener entradas para la
Olimpíada de 2012 y hubo mucha indignación porque la gente no las
recibió. La gente había hecho pedidos por cientos de libras y se les
dijo que no recibirían nada. Pero nadie sabía si eran solo unas pocas
personas las que se quejaban ruidosamente mientras la mayoría estaba
contenta. Por lo que intentamos encontrar una manera de saberlo.

Decidimos que lo mejor que podíamos hacer realmente, dado que no había
buenos datos sobre el tema, era preguntar a la gente. Y pensamos que
tendríamos que tratarlo como un tema no demasiado serio, porque no
teníamos una muestra representativa.

Creamos un formulario en Google e http://bit.ly/guardian-olympics2[hicimos preguntas muy específicas]. 
En realidad era un cuestionario largo: preguntaba cuánto era el valor de
las entradas que habían pedido, cuánto habían debitado de sus tarjetas
de crédito, qué eventos querían ver, este tipo de cosas.

[[FIG0414]]
.¿Cuántas entradas Olímpicas consiguió?: los resultados de los lectores (The Guardian)
image::figs/incoming/04-FF.png[float="0"]

Lo pusimos como una pequeña imagen a la cabeza del sitio y se difundió
rápidamente. Creo que esta es una de las cosas claves; no se puede solo
pensar “¿Qué es lo que quiero saber para mi historia?”. Hay que pensar:
“¿Qué me quiere contar la gente ahora?” Y el "crowdsourcing" es exitoso
cuando uno descubre de qué quiere hablar la gente. El volumen de
respuestas para este proyecto, que es uno de nuestros primeros intentos
de "crowdsourcing", fue inmenso. Tuvimos 1.000 respuestas en menos de
una hora y 7.000 para el final del día.

Por lo que obviamente, tomamos un poco más seriamente la presentación de
los resultados en este momento. Inicialmente no sabíamos cómo nos iba a
ir. Por lo que agregamos algunas consideraciones: los lectores del
Guardian pueden tener mayores ingresos que otra gente, la gente que
recibió menos de lo esperado podía estar más dispuesta a hablar con
nosotros, y así siguiendo.

No sabíamos cuánto valor tendrían los resultados. Terminamos con unos
7.000 registros en los cuales basar nuestro trabajo, y descubrimos que
alrededor de la mitad de la gente que pidió entradas no recibió nada.
Presentamos todo esto y debido a que tanta gente había participado el
día anterior, hubo mucho interés en los resultados.

Pocas semanas más tarde salió el informe oficial y nuestras cifras
resultaron llamativamente precisas. Eran casi exactas. Creo que en parte
fue por una cuestión de suerte, pero también porque logramos que
respondiera tanta gente.

Si uno le pregunta a sus lectores sobre algo así y contestan en los
comentarios de la nota, estará limitado en lo que puede hacer con los
resultados. De modo que tiene que empezar por pensar: “¿Cuál es la mejor
herramienta para lo que quiero saber?” ¿Es un hilo de comentarios? ¿O
tengo que crear una aplicación? Y si es crear una aplicación, hay que
pensar: “¿Vale la pena la espera? ¿Y se justifican los recursos
requeridos para hacer esto?”

En este caso pensamos en los Formularios Google. Si alguien llena el
formulario el resultado aparece como una fila en una hoja de cálculo.
Esto significa que aunque aún si se estuviera actualizando, aún si
siguieran entrando resultados, se podría abrir la hoja de cálculo y ver
todos los resultados.

Pude haber tratado de hacer el trabajo en Google, pero lo descargué a
Microsoft Excel y luego ordené la información de menor a mayor; también
encontré las entradas en las que la gente para decir lo que gastó, había
escrito los números como palabras (en vez de colocar los dígitos), y
arreglé eso. Decidí excluir lo menos posible. De modo que en vez de solo
aceptar las respuestas válidas, traté de arreglar lo que tenía. Algunos
habían usado divisas extranjeras, así que las convertí a libras, todo lo
cual fue un poco trabajoso.

Pero hice todo el análisis en pocas horas y eliminé las entradas
obviamente tontas. Mucha gente decidió decir que no había gastado nada
en entradas. Eso es un poco gracioso, pero está bien. Eran menos de cien
en más de 7.000 entradas.

También hubo unas pocas docenas de personas que ingresaron cifras
demasiado elevadas para tratar de distorsionar los resultados. Cosas
como 10.000.000 de libras. Por lo que eso me dejó con un conjunto de
datos que podía usar con los principios normales que usamos todos los
días. Hice lo que se llama una “tabla dinámica” (pivot table). Hice
algunos porcentajes. Ese tipo de cosas.

No teníamos idea del impacto que tendría el proyecto, de modo que
trabajé yo solo con el editor del blog de deportes. Juntamos cabezas y
pensamos que este podía ser un proyecto divertido. Lo hicimos, de
comienzo a fin, en 24 horas. Tuvimos la idea, a la hora del almuerzo
armamos algo, lo pusimos a la cabeza del sitio, vimos que resultaba
bastante popular, lo dejamos a la cabeza del sitio el resto del día y
presentamos los resultados online a la mañana siguiente.

Decidimos usar Google Docs porque da completo control sobre los
resultados. No necesitaba usar las herramientas analíticas de otra
gente. Lo puedo trasladar fácilmente a un software de base de datos o a
hojas de cálculo. Cuando uno usa el software de consultas de
especialistas, a menudo se ve restringido a usar las herramientas de
ellos. Si hubiésemos estado pidiendo información muy delicada, quizás
hubiésemos dudado de usar Google y pensado en hacer algo “interno”. Pero
por lo general es muy fácil incorporar Google Forms a una página de The
Guardian y para el usuario es prácticamente invisible el hecho de que
estamos usando ese formulario. Por lo que es muy conveniente.

En términos de consejos para periodistas de datos que quieren usar el
"crowdsourcing", hay que definir cosas muy específicas para consultar a
la gente. En lo posible, haga preguntas tipo “multiple choice” (elegir
entre opciones fijas). Trate de conseguir datos demográficos básicos de
a quién se dirige, de modo de ver si su muestra puede ser distorsionada.
Si está pidiendo cantidades y cosas por el estilo, trate de especificar
que requiere la información en dígitos, que tienen que usar una moneda
específica, y así. Muchos no lo harán, pero cuanto más los guíe en todo,
tanto mejor. Y siempre, siempre, agregue una ventana para comentarios
porque mucha gente llenará los otros campos pero lo que realmente quiere
es darle su opinión sobre el tema. Especialmente si se trata de algo que
tiene que ver con los consumidores o un escándalo.

&mdash; _Marianne Bouchart, Data Journalism Blog, interviewing James Ball, the Guardian_

=== Usar y compartir datos: las reglas técnicas legales, la letra chica y la realidad

En esta sección echaremos un rápido vistazo al estado de las leyes
relacionadas con datos y bases de datos, y lo que puede hacer para
ofrecer sus datos al público usando licencias comunes y herramientas
legales. No deje que nada de lo que sigue ahogue su entusiasmo por el
periodismo de datos. Las restricciones al manejo de datos por lo general
no serán una traba y fácilmente puede asegurarse de que no sean una
traba para otros que usen los datos que usted publica.

Para decir lo obvio, obtener datos nunca fue más fácil. Antes de la
publicación generalizada de datos en la red, aunque uno hubiera
identificado un conjunto de datos que necesitaba, tenía que pedir a
quien tuviera una copia que se la pusiera a disposición, lo que
posiblemente involucrara el uso del correo o una visita personal. Ahora
uno hace que su computadora le pida a la computadora del otro que le
envíe una copia. Conceptualmente es algo similar, pero usted tiene una
copia de inmediato y el otro (el creador o editor) no ha hecho nada, y
probablemente no tenga idea de que usted descargó una copia.

¿Y qué pasa cuando se trata de descargar datos con un programa (lo que a
veces se llama “scrapear”) y condiciones de uso del servicio (en
inglés Terms of Service o ToS)? Considere la frase anterior: su
navegador es justamente ese tipo de programa. Puede ser que el ToS solo
permita acceso con cierto tipo de programa. Si tiene tiempo y dinero
ilimitados para gastar en la lectura de tales documentos y quizás para
pedir asesoramiento a un abogado, hágalo sin dudar. Pero por lo general
trate de no ser un idiota: si su programa causa daño a un sitio, su red
puede ver bloqueado el acceso al sitio en cuestión y quizás usted se lo
merezca. Ahora hay mucha experiencia respecto de acceder y "scrapear"
datos en la red. Si piensa hacer esto, le será provechoso leer los
ejemplos que se dan en sitios como ScraperWiki.

Una vez que tiene datos de interés, puede interrogar, desmenuzar,
ordenar, visualizar, correlacionar y realizar cualquier tipo de análisis
que guste con su copia de los datos. Puede publicar su análisis, citando
cualquier dato. La frase hecha “los datos son libres” (en el mismo
sentido que la palabra es libre) dice mucho, o quizás sea solo una frase
hecha de los que piensan demasiado en las cuestiones legales
relacionadas con las bases de datos o en sentido aún más amplio (y
retorcido) el aspecto legal del manejo de datos.

¿Qué sucede si, siendo un periodista de datos bueno o que aspira a ser
bueno, tiene la intención de publicar no solo su análisis, incluyendo
algunos hechos o datos puntuales, sino también los conjuntos de
datos/bases de datos que usó –y a los que quizás incorporó más
información- al realizar su análisis? O quizás solo está curando datos y
no ha hecho ningún análisis (eso es bueno: el mundo necesita curadores
de datos). Si usted está usando datos recopilados por algún otro ente,
podría haber alguna complicación. (Si su base de datos ha sido armada
totalmente por usted, de todos modos lea el siguiente párrafo como
motivación para las prácticas de compartir información que aparecen en
el párrafo posterior).

Si usted está familiarizado con el modo en que el copyright limita el
trabajo creativo –si el titular del copyright no ha dado permiso para
usar un trabajo (o el trabajo está en el dominio público o su uso puede
estar cubierto por excepciones y limitaciones tal como el uso leal) y
usted usa –distribuye, realiza, etc.- el trabajo de todos modos, el
titular del copyright podría obligarlo a interrumpirlo. Aunque los datos
son libres, los conjuntos de datos pueden ser restringidos de modo muy
similar, aunque hay más variaciones en las leyes relevantes que en el
caso del copyright aplicado a obras creativas. En síntesis, una base de
datos puede estar sujeta a copyright, como obra creativa. En muchas
jurisdicciones, por “el sudor de la frente”, simplemente armar una base
de datos, incluso de modo no creativo, hace que la base de datos esté
sujeta a copyright. En Estados Unidos en particular, tiende a exigirse
un mínimo mayor de creatividad para que haya derecho de autor (Feist v.
Rural, un caso sobre una guía telefónica, es el caso clásico
estadounidense si quiere buscarlo). Pero en algunas jurisdicciones
también hay “derechos de base de datos” que restringen el uso de bases
de datos, como cosa distinta al copyright (aunque hay mucha
superposición en términos de lo que está cubierto, en particular donde
los umbrales de creatividad para la existencia de copyright son
prácticamente inexistentes). Los más conocidos de estos son los derechos
de base de datos _sui generis_ de la Unión Europea. De nuevo,
especialmente si se encuentra en Europa, quizás quiera asegurarse de que
tiene autorización antes de publicar una base de datos de otra entidad.

Obviamente tales restricciones no son la mejor manera de promover un
ecosistema de periodismo basado en datos (tampoco es algo bueno para la
sociedad en general; científicos sociales y otros le dijeron a la UE que
no lo serían antes de la aparición de los derechos _sui generis_, y
estudios realizados desde su aparición han demostrado que tenían razón).
Afortunadamente como editor de una base de datos usted puede eliminar
tales restricciones para el uso de la base de datos (suponiendo que no
contiene elementos sobre los que usted no tiene autorización para
otorgar permiso), esencialmente otorgando permiso por adelantado. Puede
hacer esto publicando su base de datos bajo una licencia pública o una
dedicatoria al dominio público, del mismo modo que muchos programadores
difunden sus códigos bajo una licencia libre y de libre acceso, de modo
que otros puedan utilizar su código (dado que el periodismo basado en
datos a menudo involucra código, no solo datos, por supuesto que usted
debe autorizar el uso de su código también, de modo que su colección de
datos y su análisis sean reproducibles). Hay muchos motivos para dar
libre acceso a sus datos. Por ejemplo, su público podría crear nuevas
visualizaciones o aplicaciones con los mismos y con las que usted puede
crear un vínculo, como hace The Guardian con su grupo en Flickr de
visualización de datos. Sus conjuntos de datos pueden combinarse con
otros conjuntos de datos para que usted y sus lectores tengan una mejor
visión de un tema. Las cosas que hacen otros con sus datos pueden darle
pistas para nuevas historias, o ideas para historias, o ideas para otros
proyectos basados en datos. Y sin duda le dará prestigio.

[[FIG0415]]
.Distintivos de datos abiertos (Open Knowledge Foundation)
image::figs/incoming/04-GG.jpg[float="none"]
 
Cuando uno advierte que difundir trabajos bajo licencias públicas es una
necesidad, la cuestión pasa a ser: ¿cuál licencia? Esa pregunta
complicada frecuentemente será respondida por el proyecto o la comunidad
en cuyo trabajo usted basa el suyo, o al que espera poder contribuir con
su trabajo: use la licencia que ellos usan. Si necesita investigar más a
fondo, empiece por el conjunto de licencias que son libres y abiertas,
es decir, que autorizan a cualquiera a darle cualquier uso (puede
requerirse tanto libertad de atribución como de compartir). La http://opendefinition.org/[Definición de Conocimiento Abierto] , en español
http://es.wikipedia.org/wiki/Conocimiento_abierto[http://es.wikipedia.org/wiki/Conocimiento_abierto],
significa para todo otro conocimiento, incluyendo las bases de datos, lo
mismo que la Definición de Software Libre y la Definición de Código
Libre significan para el software: define lo que hace que una obra sea
de libre acceso y lo que las licencias de libre acceso permiten hacer a
los usuarios.

Puede visitar el sitio de Open Knowledge Definition para ver el 
http://opendefinition.org/licenses/[actual conjunto de licencias],
algunas definiciones en español en
http://es.wikipedia.org/wiki/Licencias_Creative_Commons[Creative Commons]).
En síntesis, básicamente hay 3 clases de licencias abiertas:

Dominio Público::
  Estas también sirven como licencias de máxima permisividad; no hay
condiciones impuestas al uso de la obra.

Licencias permisivas o sólo de atribución::
  Reconocer la autoría es la única condición sustancial de estas
licencias.

Licencias copyleft, recíprocas o de compartir por igual::
  Estas también requieren que si se publican obras modificadas, sean
compartidas bajo la misma licencia.

Si usted está usando un conjunto de datos publicados por otro bajo una
licencia abierta, considere el párrafo anterior como una breve guía
respecto de cómo debe cumplir las condiciones de esa licencia abierta.
Las licencias más comunes de Creative Commons, Open data Commons y
varios gobiernos por lo general van acompañadas de una síntesis que le
permitirá ver fácilmente cuáles son las condiciones sustanciales
requeridas. Comúnmente la licencia se presentará en una página de la red
de la que puede descargarse un conjunto de datos (o de donde pueden ser
"scrapeados", ya que, por supuestos, las páginas de la red pueden
contener conjuntos de datos) o en un lugar conspicuo dentro del conjunto
de datos mismos, según el formato. Esto es lo que usted debiera hacer
también cuando autoriza el acceso a sus conjuntos de datos.

Volviendo al comienzo, ¿qué pasa si el conjunto de datos que necesita no
está disponibles online aún o hay algún tipo de control sobre los
mismos? Considere la posibilidad de pedir acceso no solo para usted,
sino que los datos se abran al uso de todo el mundo. Usted puede dar
algunas indicaciones de algunas de las grandes cosas que podrían suceder
con esos datos si así se hiciera.

El tema de compartir datos con todo el mundo podría llevar a la cuestión
de que algunos conjuntos de datos pueden afectar derechos de privacidad
y otras consideraciones y regulaciones. Por cierto, el hecho de que el
carácter abierto de la información elimina muchas barreras técnicas y de
copyright,, o del tipo del copyright no significa que no haya que
cumplir otras leyes. Pero, en el caso de que su sentido común le indique
que existe la necesidad de investigar esa cuestión, tenga en cuenta que
esto siempre fue así y que hay tremendos recursos y en algunos casos
medidas de protección para periodistas.

¡Buena suerte! Pero probablemente necesite la suerte mucho más para
otros aspectos de su proyecto que para enfrentar los (escasos) riesgos
legales.

&mdash; _Mike Linksvayer, Creative Commons_
